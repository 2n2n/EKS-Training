# Activity 3-1 Architecture - Shared EKS Cluster

This document explains the architecture of the shared EKS cluster environment where all 7 participants work together.

---

## ğŸ—ï¸ Complete Shared Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS Account (ap-southeast-1)                            â”‚
â”‚                           Managed by Root                                  â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    VPC: 10.0.0.0/16 (Shared)                         â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚   â”‚  Public Subnet A   â”‚              â”‚  Public Subnet B   â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  10.0.1.0/24       â”‚              â”‚  10.0.2.0/24       â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  AZ: 1a            â”‚              â”‚  AZ: 1b            â”‚       â”‚ â”‚
â”‚  â”‚   â”‚                    â”‚              â”‚                    â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚   Node 1     â”‚  â”‚              â”‚  â”‚   Node 2     â”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚  t3.medium   â”‚  â”‚              â”‚  â”‚  t3.medium   â”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚  Spot        â”‚  â”‚              â”‚  â”‚  Spot        â”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚  20GB gp3    â”‚  â”‚              â”‚  â”‚  20GB gp3    â”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚ All 7 Users' â”‚  â”‚              â”‚  â”‚ All 7 Users' â”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â”‚ Pods Run Hereâ”‚  â”‚              â”‚  â”‚ Pods Run Hereâ”‚  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚ â”‚
â”‚  â”‚   â”‚                    â”‚              â”‚                    â”‚       â”‚ â”‚
â”‚  â”‚   â”‚  10.0.1.50         â”‚              â”‚  10.0.2.50         â”‚       â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚   â”‚            Internet Gateway                                   â”‚ â”‚ â”‚
â”‚  â”‚   â”‚            (Public Access In/Out)                             â”‚ â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚   â”‚            Security Groups                                    â”‚ â”‚ â”‚
â”‚  â”‚   â”‚  Cluster SG: Port 443 (API access)                           â”‚ â”‚ â”‚
â”‚  â”‚   â”‚  Node SG: Inter-node + NodePort 30000-32767                  â”‚ â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            EKS Control Plane: shared-workshop-cluster                â”‚ â”‚
â”‚  â”‚            (AWS Managed - Multi-AZ, HA)                              â”‚ â”‚
â”‚  â”‚            Cost: $72/month                                           â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚   â”‚   API    â”‚    â”‚Scheduler â”‚    â”‚Controllerâ”‚   â”‚   etcd   â”‚    â”‚ â”‚
â”‚  â”‚   â”‚  Server  â”‚â”€â”€â”€â–¶â”‚          â”‚â”€â”€â”€â–¶â”‚ Manager  â”‚â—€â”€â–¶â”‚ Database â”‚    â”‚ â”‚
â”‚  â”‚   â”‚  :443    â”‚    â”‚          â”‚    â”‚          â”‚   â”‚          â”‚    â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚        â–²                                                            â”‚ â”‚
â”‚  â”‚        â”‚ kubectl commands from all 7 participants                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            Kubernetes Namespaces (Logical Isolation)                 â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   System Namespaces (Don't Touch!):                                 â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ default (system default)                                      â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ kube-system (Kubernetes components)                           â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ kube-public (public cluster info)                             â”‚ â”‚
â”‚  â”‚   â””â”€â”€ kube-node-lease (node health checks)                          â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   Participant Namespaces (Your Workspaces):                         â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ charles-workspace                                             â”‚ â”‚
â”‚  â”‚   â”‚   â””â”€â”€ [charles-webapp, charles-api, charles-db]                â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ joshua-workspace                                              â”‚ â”‚
â”‚  â”‚   â”‚   â””â”€â”€ [joshua-frontend, joshua-backend]                        â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ robert-workspace                                              â”‚ â”‚
â”‚  â”‚   â”‚   â””â”€â”€ [robert-app1, robert-app2]                               â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ sharmaine-workspace                                           â”‚ â”‚
â”‚  â”‚   â”‚   â””â”€â”€ [sharmaine-service, sharmaine-worker]                    â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ daniel-workspace                                              â”‚ â”‚
â”‚  â”‚   â”‚   â””â”€â”€ [daniel-api, daniel-frontend]                            â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ jett-workspace                                                â”‚ â”‚
â”‚  â”‚   â”‚   â””â”€â”€ [jett-app, jett-db]                                      â”‚ â”‚
â”‚  â”‚   â””â”€â”€ thon-workspace                                                â”‚ â”‚
â”‚  â”‚       â””â”€â”€ [thon-webapp, thon-service]                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            ECR Repository: eks-workshop-apps (Shared)                â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   Images (Tagged by Username):                                      â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ charles-webapp:v1, charles-webapp:v2, charles-api:v1         â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ joshua-frontend:v1, joshua-backend:v1                        â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ robert-app1:v1, robert-app2:v1                               â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ sharmaine-service:v1, sharmaine-worker:v1                    â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ daniel-api:v1, daniel-frontend:v2                            â”‚ â”‚
â”‚  â”‚   â”œâ”€â”€ jett-app:v1, jett-db:v1                                      â”‚ â”‚
â”‚  â”‚   â””â”€â”€ thon-webapp:v1, thon-service:v1                              â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   URI: <account-id>.dkr.ecr.ap-southeast-1.amazonaws.com/          â”‚ â”‚
â”‚  â”‚        eks-workshop-apps                                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            IAM Roles & Access                                        â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚   â”‚  EKS Cluster Service Role                   â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Used by: EKS Control Plane                 â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Policy: AmazonEKSClusterPolicy             â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Purpose: Manage AWS resources for cluster  â”‚                   â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚   â”‚  EKS Node Instance Role                     â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Used by: Worker Nodes (EC2)                â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Policies:                                  â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚    - AmazonEKSWorkerNodePolicy              â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚    - AmazonEKS_CNI_Policy                   â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚    - AmazonEC2ContainerRegistryReadOnly     â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Purpose: Join cluster, pull images         â”‚                   â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â”‚                                                                      â”‚ â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚ â”‚
â”‚  â”‚   â”‚  Participant IAM Users (7 users)            â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Users: eks-charles, eks-joshua, ...        â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Policy: EKSWorkshopPolicy                  â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  RBAC: Mapped to system:masters (admin)    â”‚                   â”‚ â”‚
â”‚  â”‚   â”‚  Can: Full cluster access (create/delete)   â”‚                   â”‚ â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Access Flow:
Participant â†’ AWS IAM Auth â†’ kubectl â†’ EKS API â†’ Nodes â†’ Pods (in namespace)
```

---

## ğŸ†š Shared vs Individual Setup Comparison

### Individual Setup (Activity 3)

```
Participant 1:
â””â”€â”€ VPC 1 (10.0.0.0/16)
    â””â”€â”€ EKS Cluster 1
        â””â”€â”€ 2 Nodes
            â””â”€â”€ Only Participant 1's apps

Participant 2:
â””â”€â”€ VPC 2 (10.0.0.0/16)
    â””â”€â”€ EKS Cluster 2
        â””â”€â”€ 2 Nodes
            â””â”€â”€ Only Participant 2's apps

... (5 more identical setups)

Total Resources:
â”œâ”€â”€ 7 VPCs
â”œâ”€â”€ 7 EKS Clusters
â”œâ”€â”€ 14 EC2 Nodes
â””â”€â”€ 7 ECR Repositories

Total Cost: 7 Ã— $95 = $665/month
```

### Shared Setup (Activity 3-1 - This Activity)

```
All 7 Participants:
â””â”€â”€ VPC 1 (10.0.0.0/16) [SHARED]
    â””â”€â”€ EKS Cluster 1 [SHARED]
        â””â”€â”€ 2-10 Nodes [SHARED - can scale]
            â””â”€â”€ All participants' apps (isolated by namespace)

Total Resources:
â”œâ”€â”€ 1 VPC
â”œâ”€â”€ 1 EKS Cluster
â”œâ”€â”€ 2 EC2 Nodes (initially)
â””â”€â”€ 1 ECR Repository (shared with tagging)

Total Cost: $95/month
Savings: $570/month (85% cheaper!)
```

---

## ğŸ§© Component Details

### 1. Shared VPC

**CIDR:** 10.0.0.0/16 (65,536 IP addresses)

**Why Shared:**
- One network for all participants
- Reduces complexity
- Lowers cost (no multiple IGWs, route tables)
- Real-world pattern (companies share VPCs across teams)

**What This Means for You:**
- All participants' pods run in same network
- Pods can communicate across namespaces (unless restricted)
- Network policies can add isolation if needed

---

### 2. Public Subnets (2Ã—)

**Subnet A:** 10.0.1.0/24 (256 IPs) in ap-southeast-1a
**Subnet B:** 10.0.2.0/24 (256 IPs) in ap-southeast-1b

**Why 2 Subnets:**
- High availability across availability zones
- If AZ-1a fails, AZ-1b continues
- EKS requirement (minimum 2 AZs)

**Why Public:**
- Direct internet access (no NAT Gateway cost)
- Nodes can pull container images
- Applications can receive traffic
- Good for learning/development

**Production Difference:**
- Would use private subnets + NAT Gateway
- Public subnets only for load balancers
- More secure but more expensive

---

### 3. EKS Control Plane (Shared)

**Cluster Name:** shared-workshop-cluster
**Managed by:** AWS (you don't see or manage these servers)
**High Availability:** Automatically spans 3 AZs

**Components:**

**API Server:**
- Entry point for all kubectl commands
- ALL 7 participants connect here
- Handles authentication and authorization
- Port 443 (HTTPS)

**Scheduler:**
- Decides which node runs which pod
- Considers resources, constraints, affinity
- Works for all participants' workloads

**Controller Manager:**
- Maintains desired state
- Auto-restarts failed pods
- Manages deployments, replicasets
- Works for everyone's resources

**etcd:**
- Database storing cluster state
- Contains all resources from all namespaces
- Automatically backed up by AWS

**Cost:** $0.10/hour = $72/month (fixed, regardless of users)

---

### 4. Worker Nodes (Shared)

**Initial Setup:**
- 2Ã— t3.medium Spot instances
- Distributed across both availability zones
- 2 vCPU, 4GB RAM each
- 20GB gp3 storage each

**Shared Usage:**
- ALL participants' pods run on these same nodes
- Kubernetes scheduler distributes pods
- Resource limits prevent one user hogging resources

**Scaling:**
- Configured: min=2, max=10, desired=2
- Participants can add node groups
- Coordinate before scaling to avoid surprise costs!

**What Runs on Nodes:**
- System pods (kube-proxy, aws-node, coredns)
- Participant workloads (from all namespaces)
- Each node can host ~10-15 small pods

**Resource Capacity (Per Node):**
```
CPU: 2 vCPUs
â”œâ”€â”€ System reserved: ~0.2 vCPU
â”œâ”€â”€ System pods: ~0.1 vCPU
â””â”€â”€ Available for apps: ~1.7 vCPU

Memory: 4 GB
â”œâ”€â”€ System reserved: ~0.5 GB
â”œâ”€â”€ System pods: ~0.3 GB
â””â”€â”€ Available for apps: ~3.2 GB
```

**Total Cluster (2 nodes):**
```
Available for all 7 participants:
â”œâ”€â”€ CPU: ~3.4 vCPU
â”œâ”€â”€ Memory: ~6.4 GB
â””â”€â”€ ~20-30 small pods total

Per participant (if shared equally):
â”œâ”€â”€ CPU: ~0.5 vCPU
â”œâ”€â”€ Memory: ~900 MB
â””â”€â”€ ~3-4 small pods
```

---

### 5. Namespace Isolation

**System Namespaces (Pre-existing - Don't Touch!):**

**default:**
- Default namespace for resources without namespace
- Best practice: Don't use for participant work

**kube-system:**
- Kubernetes system components
- Contains kube-proxy, coredns, aws-node
- âš ï¸ NEVER delete or modify!

**kube-public:**
- Publicly readable cluster information
- Used for cluster discovery

**kube-node-lease:**
- Node heartbeat information
- Performance optimization for node status

---

**Participant Namespaces (You Create These):**

Each participant should create personal namespace(s):

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: charles-workspace
  labels:
    owner: charles
    team: workshop
```

**Namespace Benefits:**
- Logical isolation of resources
- Organize your applications
- Apply resource quotas (optional)
- Clean up easily (delete namespace = delete all your apps)

**Namespace Limitations:**
- âš ï¸ NOT security boundaries (without network policies)
- All participants can see all namespaces
- All participants can access resources in any namespace
- Communication and trust required!

---

### 6. Shared ECR Repository

**Repository Name:** eks-workshop-apps
**URI:** `<account-id>.dkr.ecr.ap-southeast-1.amazonaws.com/eks-workshop-apps`

**Tagging Convention:**
```
Format: <username>-<appname>-<version>

Examples:
- charles-webapp-v1
- charles-webapp-v2
- charles-api-v1
- joshua-frontend-v1
- robert-backend-v2
```

**Why One Shared Repository:**
- Simpler permission management
- Lower cost (charged per repository)
- Easy to see all workshop images
- Teaches tagging discipline

**Best Practices:**
- Always prefix with your username
- Use semantic versioning (v1, v2, v3)
- Document what each image contains
- Clean up old images

---

### 7. IAM and RBAC Permissions

**Two Permission Systems:**

**IAM (AWS Level):**
- Controls access to AWS APIs
- Participants have EKSWorkshopPolicy
- Grants: EKS, EC2, ECR, CloudFormation, etc.
- Allows cluster access and node management

**RBAC (Kubernetes Level):**
- Controls access within the cluster
- Participants mapped to `system:masters` group
- Full administrative access to all resources
- Can create/delete anything in cluster

**Permission Flow:**
```
1. Participant runs: kubectl get pods
2. IAM checks: Does user have eks:DescribeCluster?
3. kubectl connects to API server
4. API server checks: What RBAC role does user have?
5. RBAC: system:masters = full access
6. API server returns pod list
```

**Security Implications:**

âš ï¸ **Full Admin Access Means:**
- You CAN delete the entire cluster
- You CAN delete other participants' resources
- You CAN consume all cluster resources
- You CAN modify system components

**Why This Setup:**
- Learning environment (not production)
- Trust-based model
- Full hands-on experience
- Teaches responsibility

**Production Alternative:**
- Create namespace-specific RBAC roles
- Limit users to their own namespaces
- Separate node management permissions
- Audit all actions with CloudTrail

---

## ğŸ“Š Resource Distribution

### How Resources Are Shared

**CPU and Memory:**
```
Total Available: 3.4 vCPU, 6.4 GB RAM

Without Resource Limits:
â””â”€â”€ First-come, first-served
    â””â”€â”€ One user could consume everything!

With Resource Limits (Recommended):
â””â”€â”€ Each pod specifies requests and limits
    â””â”€â”€ Scheduler ensures fair distribution
```

**Example Pod with Limits:**
```yaml
resources:
  requests:     # Guaranteed resources
    cpu: 100m
    memory: 128Mi
  limits:       # Maximum allowed
    cpu: 200m
    memory: 256Mi
```

---

### Storage Distribution

**Node Storage (EBS):**
- Each node: 20GB gp3
- Shared by all pods on that node
- System takes ~4GB
- Container images take ~2GB per node
- Remaining ~14GB for pod data

**Persistent Volumes (If Used):**
- Created separately from nodes
- Can be provisioned by any participant
- Charged separately ($0.08/GB/month)
- Should be deleted when not needed

---

## ğŸ’° Detailed Cost Breakdown

### Fixed Costs (Always Running)

```
EKS Control Plane:
â”œâ”€â”€ $0.10/hour
â”œâ”€â”€ $2.40/day
â”œâ”€â”€ $72/month
â””â”€â”€ Shared by all 7 participants = $10.29/person/month
```

### Variable Costs (Based on Usage)

```
Worker Nodes (2Ã— t3.medium Spot):
â”œâ”€â”€ On-Demand: $0.0416/hour each = $60/month for both
â”œâ”€â”€ Spot (70% off): $0.0125/hour each = $18/month for both
â””â”€â”€ Shared by 7 participants = $2.57/person/month

EBS Volumes (2Ã— 20GB gp3):
â”œâ”€â”€ $0.08/GB/month
â”œâ”€â”€ 40GB total = $3.20/month
â””â”€â”€ Shared by 7 participants = $0.46/person/month

ECR Storage:
â”œâ”€â”€ $0.10/GB/month
â”œâ”€â”€ Estimated 5GB total images = $0.50/month
â””â”€â”€ Shared by 7 participants = $0.07/person/month

Data Transfer:
â”œâ”€â”€ Inbound: Free
â”œâ”€â”€ Outbound: $0.09/GB (first 1GB free/month)
â”œâ”€â”€ Estimated: ~$0.50/month
â””â”€â”€ Shared by 7 participants = $0.07/person/month

CloudWatch Logs (Optional):
â”œâ”€â”€ $0.50/GB ingested
â”œâ”€â”€ $0.03/GB stored
â”œâ”€â”€ Short retention = ~$1-2/month
â””â”€â”€ Shared by 7 participants = $0.14-0.29/person/month
```

### Total Costs

```
Total Cluster Cost:
â”œâ”€â”€ Fixed: $72/month
â”œâ”€â”€ Variable: $23/month
â””â”€â”€ Total: ~$95/month

Per Participant (if split equally):
â””â”€â”€ $95 Ã· 7 = $13.57/month/person

For 4-hour Workshop:
â”œâ”€â”€ Total: $0.13/hour Ã— 4 = $0.52
â””â”€â”€ Per person: $0.52 Ã· 7 = $0.07/person!
```

### Cost Comparison

```
Individual Setup (Activity 3):
â”œâ”€â”€ 7 clusters Ã— $95 = $665/month
â””â”€â”€ 4-hour workshop: $3.64 total

Shared Setup (This Activity):
â”œâ”€â”€ 1 cluster = $95/month
â””â”€â”€ 4-hour workshop: $0.52 total

Savings:
â”œâ”€â”€ Monthly: $570 (85% cheaper!)
â””â”€â”€ 4-hour workshop: $3.12 (85% cheaper!)
```

---

## ğŸ”„ Data Flow Examples

### 1. Participant Deploys Application

```
1. Charles â†’ kubectl apply -f webapp.yaml -n charles-workspace
   â†“
2. kubectl â†’ AWS IAM authentication (validates eks-charles user)
   â†“
3. Sends HTTPS request to EKS API Server (port 443)
   â†“
4. API Server â†’ Authenticates via aws-auth ConfigMap
   â†“
5. API Server â†’ Checks RBAC (system:masters = allowed)
   â†“
6. API Server â†’ Validates YAML, stores in etcd
   â†“
7. Controller Manager â†’ Sees new Deployment
   â†“
8. Creates ReplicaSet with 2 pods
   â†“
9. Scheduler â†’ Assigns pods to nodes (node 1 and node 2)
   â†“
10. kubelet on each node â†’ Pulls image from ECR
    â†“
11. kubelet â†’ Starts containers
    â†“
12. Pods running in charles-workspace namespace!
```

---

### 2. Participant Accesses Another's Application

```
Scenario: Joshua wants to call Charles's API

1. Joshua's pod (in joshua-workspace)
   â†“
2. DNS lookup: charles-api.charles-workspace.svc.cluster.local
   â†“
3. CoreDNS resolves to ClusterIP (e.g., 10.100.23.45)
   â†“
4. Packet sent to ClusterIP
   â†“
5. kube-proxy (iptables) on local node routes to charles-api pod
   â†“
6. Packet reaches Charles's pod (could be on any node)
   â†“
7. Charles's API responds
   â†“
8. Response flows back to Joshua's pod

Note: This works because there's NO network isolation between namespaces!
```

---

### 3. External User Accesses Application

```
1. User â†’ http://54.123.45.67:30080 (NodePort service)
   â†“
2. Internet â†’ AWS Internet Gateway
   â†“
3. Routes to VPC
   â†“
4. Security Group check (port 30080 allowed?)
   â†“
5. Reaches Node's public IP
   â†“
6. kube-proxy (iptables) routes to target pod
   â†“
7. Pod could be in ANY namespace (charles, joshua, etc.)
   â†“
8. Application processes request
   â†“
9. Response flows back through same path
```

---

### 4. Participant Pushes Image to ECR

```
1. Charles â†’ docker build -t myapp .
   â†“
2. Docker builds image locally
   â†“
3. Charles â†’ aws ecr get-login-password | docker login
   â†“
4. AWS CLI authenticates with IAM (eks-charles credentials)
   â†“
5. Returns temporary ECR auth token
   â†“
6. Docker stores auth token
   â†“
7. Charles â†’ docker tag myapp <ecr-uri>:charles-myapp-v1
   â†“
8. Image tagged with naming convention
   â†“
9. Charles â†’ docker push <ecr-uri>:charles-myapp-v1
   â†“
10. Docker uploads layers to ECR
    â†“
11. ECR stores image in eks-workshop-apps repository
    â†“
12. All participants can now pull: charles-myapp-v1
```

---

## âš¡ Coordination Patterns

### Best Practices for Shared Environment

**1. Naming Conventions:**
```
Namespaces:      <name>-workspace (charles-workspace)
Deployments:     <name>-<app> (charles-webapp)
Services:        <name>-<app>-svc (charles-webapp-svc)
Node Groups:     <name>-nodes (charles-nodes)
ECR Images:      <name>-<app>-<version> (charles-webapp-v1)
```

**2. Communication Protocol:**
- Announce in team chat before:
  - Creating node groups
  - Scaling beyond 4 nodes total
  - Deleting node groups
  - Creating large deployments (>5 replicas)

**3. Resource Etiquette:**
```
DO:
âœ… Set resource requests/limits on pods
âœ… Use appropriate replica counts (2-3 for learning)
âœ… Delete resources when done testing
âœ… Monitor cluster capacity
âœ… Ask before scaling nodes

DON'T:
âŒ Create 20 replicas of your app
âŒ Deploy without resource limits
âŒ Delete others' resources
âŒ Use all available capacity
âŒ Leave resources running overnight
```

**4. Conflict Avoidance:**
- Use personal namespaces
- Prefix all resources with your name
- Check cluster capacity before deploying
- Coordinate node group changes
- Clean up after testing

---

## ğŸ¯ Success Criteria

Your shared cluster is working correctly when:

```
Infrastructure (Root Setup):
âœ… VPC with 2 subnets exists
âœ… Internet Gateway attached
âœ… Security groups configured
âœ… EKS cluster Status = Active
âœ… 2 worker nodes Ready
âœ… ECR repository created
âœ… All 7 participants can connect

Participant Usage:
âœ… Each participant can kubectl get nodes
âœ… Each has created personal namespace
âœ… Apps deployed in personal namespaces
âœ… Images pushed to ECR with name prefixes
âœ… No resource conflicts
âœ… Good communication among participants
```

---

## ğŸ“š What You're Learning

### Technical Skills

**Kubernetes Multi-Tenancy:**
- Namespace isolation
- Resource quotas (optional)
- RBAC permissions
- Network policies (optional)

**Collaboration:**
- Working in shared infrastructure
- Coordinating resource usage
- Communication protocols
- Conflict resolution

**AWS Skills:**
- Shared VPC patterns
- IAM authentication with EKS
- ECR image management
- Cost optimization

### Real-World Parallels

This setup mirrors real production environments:

```
Your Workshop:                  Real Company:
â”œâ”€â”€ Shared cluster             â”œâ”€â”€ Shared cluster per environment
â”œâ”€â”€ 7 participants             â”œâ”€â”€ 10-100 developers
â”œâ”€â”€ Personal namespaces        â”œâ”€â”€ Team namespaces
â”œâ”€â”€ Coordinate scaling         â”œâ”€â”€ Capacity planning
â”œâ”€â”€ Communication required     â”œâ”€â”€ Infrastructure team coordination
â””â”€â”€ Cost awareness             â””â”€â”€ FinOps practices
```

---

## ğŸš€ Next Steps

After understanding this architecture:

1. **Root:** Follow ROOT-SETUP guides to build this infrastructure
2. **Participants:** Follow PARTICIPANT-GUIDES to use the cluster
3. **Everyone:** Read SAFETY-GUIDELINES.md
4. **Everyone:** Practice good communication!

---

## ğŸ”— Related Documents

- [README.md](README.md) - Start here for overview
- [SAFETY-GUIDELINES.md](SAFETY-GUIDELINES.md) - Critical reading!
- [ROOT-SETUP/](ROOT-SETUP/) - Admin setup guides
- [PARTICIPANT-GUIDES/](PARTICIPANT-GUIDES/) - User guides
- [REFERENCE/](REFERENCE/) - Command references and troubleshooting

---

**Remember:** This is a SHARED environment. Your actions affect 6 other people. Be thoughtful, communicate, and have fun learning together! ğŸ¤

